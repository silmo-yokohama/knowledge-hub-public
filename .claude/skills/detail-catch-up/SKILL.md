---
name: detail-catch-up
description: /daily-trendsレポートでチェックした記事を詳細に分析し、要約・深掘り・議論分析を含むレポートを生成する。「詳細分析」「深掘り」で起動。
---

# /detail-catch-up - 記事詳細分析レポート生成スキル

## 概要

`/daily-trends` で生成されたレポートでユーザーがチェックを入れた記事を詳細に分析し、
要約・深掘り・議論分析を含む個別レポートを生成する。

## ワークフロー

以下のステップを順番に実行してください。

### Step 1: Headlinesレポートの読み込み

1. PROFILE.md を読み込む: `/home/a/00.knowledge-hub/PROFILE.md`
2. 最新のHeadlinesレポート（JSON形式）を探す

**レポートの探し方**:
- まず `01.Trends/Headlines/` 配下のディレクトリをGlobツールで確認する
- パターン: `/home/a/00.knowledge-hub/01.Trends/Headlines/**/*.json`
- 最新の日付のファイルを選択する
- ユーザーが特定の日付を指定した場合はそのファイルを使用する

3. レポートファイル（JSON）をReadツールで読み込み、パースする

### Step 2: チェック済み記事の検出

読み込んだJSONデータの `articles` 配列から `checked: true` の記事を抽出する。

**抽出方法**:
```
articles.filter(article => article.checked === true)
```

各チェック済み記事から以下を取得:
- `title`: 記事タイトル
- `url`: 記事URL
- `category`: カテゴリー
- `source`: 取得元（`"hatena"` / `"yahoo"` / `"reddit"`）
- `rank`: ランク（`"S"` / `"A"` / `"B"` / `"C"`）

### Step 3: 分析対象の確認

チェック済み記事の一覧をユーザーに表示し、分析開始の確認を取る。

表示形式:
```
以下の {N} 件の記事を詳細分析します:

1. [記事タイトル](URL) - {ランク}
2. [記事タイトル](URL) - {ランク}
...

分析を開始してよろしいですか？
```

チェック済み記事が0件の場合:
```
チェックされた記事が見つかりませんでした。
ビューア（cd viewer && npm run dev）で記事をチェックしてから再度実行してください。
```

### Step 4: 各記事の詳細分析

チェック済みの各記事に対して、以下の処理を実行する。
**重要**: 各記事の処理の間に1秒以上のスリープ（`sleep 1`）を入れ、外部APIへの負荷を分散すること。

#### 4-1. 記事本文の取得

- **WebFetch** ツールで記事URLから本文を取得する
  - prompt: "この記事の全文を取得してください。タイトル、本文、引用、コードブロックを含めて、できるだけ完全な内容を返してください。"
- WebFetchで取得できない場合（JavaScript依存サイト等）は **WebSearch** で記事タイトルを検索し、代替情報を収集する
- 記事内の参照リンク・引用元がある場合、最大3件まで追加でWebFetchで取得する

#### 4-2. 要約の生成

取得した記事内容から以下を生成:
- **記事の要約**: 箇条書きで記事の主要ポイントを3〜5項目にまとめる
- **詳細の深掘り**: ポイントごとに段落を分けて深く掘り下げる
- **キーポイント**: 特に重要な点を箇条書き3〜5項目

#### 4-3. PROFILE.md視点での深掘り

PROFILE.mdの内容を踏まえて:
- **実務への応用**: フリーランスフロントエンドエンジニアとしての業務にどう関連するか
- **学習ヒント**: 「深掘りしたい分野」「まだ詳しくない分野」との関連性があれば記載

#### 4-4. コメント取得・議論分析

取得元に応じて適切なスクリプトでコメントを取得し、`references/discussion-analysis.md` の方法論に従い議論を分析する。

##### 取得元がはてブの場合

Bashツールで以下のコマンドを実行:

```bash
python3 /home/a/00.knowledge-hub/scripts/fetch_hatena_comments.py "{記事URL}"
```

##### 取得元がYahoo ニュースの場合

Bashツールで以下のコマンドを実行:

```bash
python3 /home/a/00.knowledge-hub/scripts/fetch_yahoo_comments.py "{記事URL}"
```

- このスクリプトはYahoo ニュースの公開APIから全コメントを自動取得する（ページネーション対応済み）
- 記事URLは `/comments` 付きでも `/comments` なしでもどちらでも動作する
- ページ間には1秒のスリープが自動的に入る

##### 記事URLがRedditの場合（取得元を問わず）

記事URLに `reddit.com/r/` が含まれる場合、取得元（はてブ・Yahoo ニュース）に関わらず、Bashツールで以下のコマンドを実行:

```bash
python3 /home/a/00.knowledge-hub/scripts/fetch_reddit_comments.py "{記事URL}"
```

- WebFetchはreddit.comをブロックするため、このスクリプトを使用する
- `old.reddit.com` と `www.reddit.com` の両方に対応
- ネストされた返信コメントもフラット化して取得する
- スコア順（best）でソートして取得する

**注意**: Redditの記事本文（selftext）はスクリプトの出力に含まれるため、WebFetchの代わりにこのスクリプトの出力を記事本文の取得にも活用できる。

**分析スキップ条件**:
- はてブの場合: `comments_count` が10件未満
- Yahoo ニュースの場合: `total_comments` が10件未満
- Redditの場合: `total_comments` が10件未満
- スクリプト実行が失敗した場合

#### 4-5. 関連リンクの収集

- 記事内で引用されているリンクを抽出
- WebFetchで取得した引用先のタイトルとURLをリストアップ
- 関連リンクが見つからない場合はセクションを省略する

### Step 5: レポートファイルの保存

各記事につき1ファイルで、`references/analysis-template.md` のテンプレートに従い保存する。

**保存先パス**:
```
/home/a/00.knowledge-hub/01.Trends/DeepDives/YYYY-MM/YYYY-MM-DD_記事のタイトル.md
```

- `YYYY-MM` ディレクトリが存在しない場合は作成する
- ファイル名には記事の元タイトルをそのまま使用する
- ファイル名に使えない文字（`/`, `\`, `:`, `*`, `?`, `"`, `<`, `>`, `|`）は全角に変換する
- 英語のタイトルはそのまま使用する（翻訳しない）
- 同名ファイルが既に存在する場合はユーザーに確認する

### Step 6: 完了報告

全記事の分析が完了したら、以下を報告する:

```
詳細分析が完了しました。

生成したレポート:
1. {ファイルパス} - {記事タイトル}
2. {ファイルパス} - {記事タイトル}
...

エラーがあった記事: {あれば記載}
```

## 注意事項

- レポートは全て日本語で記述する
- **英語のタイトルや本文は日本語に翻訳して出力する**
- 既存ファイルの上書きは禁止（ユーザー確認必須）
- 記事本文が全く取得できない場合は、タイトルと概要情報のみで簡易分析を行い、その旨を明記する
- 各記事間で1秒のスリープを入れ、外部APIへの負荷を分散する
- 議論分析は中立的な立場で行い、個人攻撃や差別的発言は分析から除外する
- `AI要約` タグがついたブコメは自動生成の可能性があるため分析対象から除外する
